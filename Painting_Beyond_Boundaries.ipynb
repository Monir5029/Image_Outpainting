{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j5poQFgbVHP3"
   },
   "source": [
    "# <center> <font color=green> PAINTING BEYOND IMAGE BOUNDARIES USING </font> <font color=red>GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C5KzNyiYVHP5"
   },
   "source": [
    "# References:\n",
    " - https://arxiv.org/abs/1808.08483 [1]\n",
    " - https://github.com/ShinyCode/image-outpainting [2]\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pm9qUzezVHP6"
   },
   "source": [
    "# <center> <font color=blue> Necessary Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4862,
     "status": "ok",
     "timestamp": 1575047097736,
     "user": {
      "displayName": "Md Moniruzzaman Monir",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDq1BCXeVH11taJzy5lERdkWJB49Ci93tXcHV7l=s64",
      "userId": "08480300026192839653"
     },
     "user_tz": 240
    },
    "id": "pwbYBGrRVHP7",
    "outputId": "a943c730-9ec5-4c56-eeb3-d430e36fd89e"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import numpy as np\n",
    "from numpy import asarray\n",
    "from numpy import cov\n",
    "from numpy import trace\n",
    "from numpy import iscomplexobj\n",
    "from numpy.random import shuffle\n",
    "from numpy.random import randint\n",
    "from numpy.random import randn\n",
    "from numpy.random import random\n",
    "\n",
    "from scipy.linalg import sqrtm\n",
    "from skimage.transform import resize\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import copy\n",
    "import keras\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "from keras.preprocessing import image\n",
    "\n",
    "# Models -> Layers -> Initializers -> Optimizers -> Utility function\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Conv2D, Conv2DTranspose, Dropout\n",
    "from keras.layers import Activation, LeakyReLU, BatchNormalization, Flatten, Reshape\n",
    "from keras.layers.convolutional import AtrousConvolution2D # needed for dilated convolutional layer in \"Generator\"\n",
    "\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import plot_model, np_utils\n",
    "from contextlib import redirect_stdout  # for writing model.summary into a text file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Of9NkWVmVHP9"
   },
   "source": [
    "# <center> Drive Folder - Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 37897,
     "status": "ok",
     "timestamp": 1575047130868,
     "user": {
      "displayName": "Md Moniruzzaman Monir",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDq1BCXeVH11taJzy5lERdkWJB49Ci93tXcHV7l=s64",
      "userId": "08480300026192839653"
     },
     "user_tz": 240
    },
    "id": "8Qlbqb1nVHP-",
    "outputId": "74283f79-d283-424d-dd86-bfb013a91f23"
   },
   "outputs": [],
   "source": [
    "# For integrating drive with colab\n",
    "from google.colab import files\n",
    "\n",
    "#  Google Drive Authentication\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oeu5OO1fVHQA"
   },
   "outputs": [],
   "source": [
    "path = '/content/drive/My Drive/OutPaint_DCGAN/'\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rtCUFw9zVHQC"
   },
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.ERROR)  # suppressing warning messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P3B0E7I-VHQD"
   },
   "source": [
    "#  <center>Dataset: <font color=blue> places365 </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mDiaTNWgVHQE"
   },
   "source": [
    " - Dataset URL : http://data.csail.mit.edu/places/places365/val_256.tar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NASz1ybXVHQF"
   },
   "source": [
    "### Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ii7Lxj_7VHQF"
   },
   "outputs": [],
   "source": [
    "# Ref [2], I took the idea for the below function from their implementation and then write the code by my own\n",
    "\n",
    "def create_maksed_images(X_temp):\n",
    "    # Mask creation\n",
    "    image_count = X_temp.shape[0]\n",
    "    mask_shape  = (image_count,128,128,1)\n",
    "    mask = np.zeros(mask_shape)\n",
    "    mask[:, :, :32, :] = 1.0 # left portion\n",
    "    mask[:, :,-32:, :] = 1.0 # right portion\n",
    "    \n",
    "    # Filled the left and right portion with the pixel_average (mu)\n",
    "    mu = np.mean(X_temp, axis=(1,2,3))\n",
    "\n",
    "    X_temp[:, :, :32, :] = mu[:, np.newaxis, np.newaxis, np.newaxis]\n",
    "    X_temp[:, :,-32:, :] = mu[:, np.newaxis, np.newaxis, np.newaxis]\n",
    "    X_mask = np.concatenate((X_temp, mask), axis=3)\n",
    "    \n",
    "    return X_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KRbvQMjYVHQH"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('places_dataset.npz'):\n",
    "    \n",
    "    image_size = (128,128,3)\n",
    "    raw_images_path = '/content/drive/My Drive/OutPaint_DCGAN/raw_dataset/val_2000/*.jpg'  # Use 2000 images as there was getting an error  \"A Google Drive timeout has occurred\" due to too many file accesses\n",
    "    \n",
    "    raw_images = [cv2.imread(file) for file in glob.glob(raw_images_path)]\n",
    "    raw_images = np.asarray(raw_images) \n",
    "    print(\"Loaded all images from disk\")\n",
    "    resized_images = list()\n",
    "\n",
    "    for image in raw_images:\n",
    "        new_image = resize(image, image_size, anti_aliasing=True)\n",
    "        resized_images.append(new_image)\n",
    "        \n",
    "    resized_images = np.asarray(resized_images)\n",
    "    print(\"Resized all images\")\n",
    "\n",
    "    # All training images(normalized) and size of every image : 128 by 128 by 3\n",
    "    X_train = resized_images/255.0 \n",
    "    X_temp  = copy.deepcopy(X_train)\n",
    "\n",
    "    # All processed and masked training images and size of each image : 128 by 128 by 4\n",
    "    X_mask = create_maksed_images(X_temp)\n",
    "    np.savez('places_dataset.npz', X_train=X_train, X_mask=X_mask) \n",
    "    print(\"Save processed images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f-SxSkupVHQI"
   },
   "outputs": [],
   "source": [
    "data = np.load('places_dataset.npz')\n",
    "X_train = data['X_train']\n",
    "X_mask  = data['X_mask']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "al9dIGIyVHQK"
   },
   "source": [
    "# <center> <font color=red>Global</font> <font color=blue>  Discriminator - Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 50550,
     "status": "ok",
     "timestamp": 1575047143989,
     "user": {
      "displayName": "Md Moniruzzaman Monir",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDq1BCXeVH11taJzy5lERdkWJB49Ci93tXcHV7l=s64",
      "userId": "08480300026192839653"
     },
     "user_tz": 240
    },
    "id": "9EnM3bbvVf-y",
    "outputId": "8ebe985c-3302-40fc-afb8-fb1a68496c74"
   },
   "outputs": [],
   "source": [
    "    '''\n",
    "       Model Architecture from Paper : Ref: [1]\n",
    "      \n",
    "       Layer-Type  Filter-Size     Stride    No_of_filters\n",
    "          ----     -----------     ------    -------------\n",
    "          CONV          5            2           32\n",
    "          CONV          5            2           64\n",
    "          CONV          5            2           64\n",
    "          CONV          5            2           64\n",
    "          CONV          5            2           64\n",
    "          FC            -            -           512\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JlHiuqY_VHQL"
   },
   "outputs": [],
   "source": [
    "# Use strided convolutions to repeatedly downsample an image for binary classification\n",
    "\n",
    "def create_D():\n",
    "    \n",
    "    D    = Sequential(name='Global_Discriminator')\n",
    "    init = RandomNormal(stddev=0.02) # zero-centered Gaussian distribution with a standard deviation of 0.02.\n",
    "  \n",
    "    \n",
    "    D.add(Conv2D(32, (5,5), strides=2,padding='same',\n",
    "                 kernel_initializer=init, input_shape=(128,128,3), activation='relu'))\n",
    "    \n",
    "    \n",
    "    D.add(Conv2D(64, (5,5), strides=2, padding='same', activation='relu'))\n",
    "\n",
    "    D.add(Conv2D(64, (5,5), strides=2, padding='same', activation='relu'))\n",
    "    D.add(BatchNormalization(momentum=0.9))\n",
    "\n",
    "    D.add(Conv2D(64, (5,5), strides=2, padding='same', activation='relu'))\n",
    "    D.add(BatchNormalization(momentum=0.9))\n",
    "    \n",
    "  \n",
    "    D.add(Conv2D(64, (5,5), strides=2, padding='same', activation='relu'))\n",
    "    D.add(BatchNormalization(momentum=0.9))\n",
    "\n",
    "    # Final Layer : Fully Connected Layer (classifier) \n",
    "    D.add(Flatten())\n",
    "    D.add(Dropout(0.4))\n",
    "    \n",
    "    D.add(Dense(512, activation='relu'))  \n",
    "    D.add(Dense(1,   activation='sigmoid'))  \n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FLerbAgHVHQN"
   },
   "outputs": [],
   "source": [
    "# plot_model(create_D())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lSL2SLINVHQP"
   },
   "outputs": [],
   "source": [
    "#Saving the architecture of the Discriminator model in a file in google drive\n",
    "\n",
    "if not os.path.exists('Global_Discriminator.txt'):\n",
    "    with open('Global_Discriminator.txt', 'w') as f:\n",
    "        with redirect_stdout(f):\n",
    "            create_D(0.2,0.02).summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qRy5TEzGVHQR"
   },
   "source": [
    "# <center><font color=blue> Generator - Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8cXvDdAtVHQR"
   },
   "outputs": [],
   "source": [
    "# Encode-Decoder CNN\n",
    "\n",
    "def create_G():\n",
    "    \n",
    "    G    = Sequential(name='Generator')\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    \n",
    "      \n",
    "    G.add(Conv2D(64, (5,5), strides=1, dilation_rate=(1,1), padding='same', \n",
    "                              kernel_initializer=init, input_shape = (128,128,4), activation='relu' ))\n",
    "   \n",
    "\n",
    "    G.add(Conv2D(128, (3,3), strides=2, dilation_rate=(1,1), padding='same', activation='relu')) \n",
    "    G.add(Conv2D(256, (3,3), strides=1, dilation_rate=(1,1), padding='same', activation='relu'))\n",
    "  \n",
    "     \n",
    "    G.add(Conv2D(256, (3,3), strides=1, dilation_rate=(2,2), padding='same', activation='relu'))\n",
    "    G.add(BatchNormalization(momentum=0.9))\n",
    "\n",
    "    G.add(Conv2D(256, (3,3), strides=1, dilation_rate=(4,4), padding='same', activation='relu'))\n",
    "    G.add(BatchNormalization(momentum=0.9))\n",
    "    \n",
    "    G.add(Conv2D(256, (3,3), strides=1, dilation_rate=(8,8), padding='same', activation='relu'))\n",
    "    G.add(BatchNormalization(momentum=0.9))\n",
    "    \n",
    "    G.add(Conv2D(256, (3,3), strides=1, dilation_rate=(1,1), padding='same', activation='relu'))\n",
    "    G.add(BatchNormalization(momentum=0.9))\n",
    "    \n",
    "    \n",
    "    \n",
    "    G.add(Conv2DTranspose(128,(4,4),strides=2, padding='same', activation='relu')) # DECONV\n",
    "    G.add(BatchNormalization(momentum=0.9))\n",
    "    \n",
    "    G.add(Conv2D(64, (3,3), strides=1, dilation_rate = (1,1), padding='same', activation='relu'))\n",
    "    G.add(BatchNormalization(momentum=0.9))\n",
    "    \n",
    "    G.add(Conv2D(3, (3,3), strides=1, padding='same', activation='sigmoid'))\n",
    "    \n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CP8qgAfqVHQT"
   },
   "outputs": [],
   "source": [
    "# plot_model(create_G())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_5Vt_e2dVHQV"
   },
   "outputs": [],
   "source": [
    "# Saving the architecture of the Generator model in a file in drive\n",
    "\n",
    "if not os.path.exists('Generator.txt'):\n",
    "    with open('Generator.txt', 'w') as f:\n",
    "        with redirect_stdout(f):\n",
    "            create_G().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R6p9-SdwVHQW"
   },
   "source": [
    "# <center><font color=blue> GAN - Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WDiEBxq9VHQX"
   },
   "outputs": [],
   "source": [
    "# custom loss functions \n",
    "\n",
    "def G_loss_MSE(y_true, y_pred):\n",
    "    diff = y_pred - y_true\n",
    "    loss = K.mean(K.square(diff))\n",
    "    return loss\n",
    "\n",
    "def G_loss(y_true, y_pred):\n",
    "    \n",
    "    mse_loss = G_loss_MSE(y_true, y_pred)\n",
    "    \n",
    "    fake_prediction = tf.maximum(y_pred, K.epsilon())\n",
    "    fake_loss = tf.log(fake_prediction)\n",
    "    \n",
    "    loss = mse_loss - 0.004 * tf.reduce_mean(fake_loss)  # 0.004 is hyperparameter from the paper\n",
    "    return loss\n",
    "\n",
    "def D_loss(y_true, y_pred):\n",
    "    \n",
    "    real_prediction = tf.maximum(y_true, K.epsilon())\n",
    "    fake_prediction = tf.maximum(1.0 - y_pred, K.epsilon())\n",
    "    \n",
    "    real_loss = tf.log(real_prediction)\n",
    "    fake_loss = tf.log(fake_prediction)\n",
    "    \n",
    "    loss = -tf.reduce_mean(real_loss+fake_loss)\n",
    "    return loss\n",
    "\n",
    "def wasserstein_loss(y_true, y_pred):\n",
    "    return K.mean(y_true * y_pred)\n",
    "\n",
    "\n",
    "def create_DCGAN(g_learning_rate, g_beta_1, d_learning_rate, d_beta_1, phase=1):\n",
    "    \n",
    "    # discriminator\n",
    "    D = create_D()\n",
    "    D_optimizer = Adam(lr=d_learning_rate, beta_1=d_beta_1)\n",
    "    D.compile(optimizer=D_optimizer,loss=D_loss, metrics=['binary_accuracy'])\n",
    "    \n",
    "  \n",
    "    # generator\n",
    "    G = create_G()\n",
    "\n",
    "    # GAN\n",
    "    gan = Sequential([G,D])\n",
    "    gan_optimizer = Adam(lr=g_learning_rate, beta_1=g_beta_1)\n",
    "    \n",
    "    if phase==1:\n",
    "        gan.compile(optimizer=gan_optimizer, loss=G_loss_MSE, metrics=['binary_accuracy'])\n",
    "    else:\n",
    "        gan.compile(optimizer=gan_optimizer, loss=G_loss, metrics=['binary_accuracy'])\n",
    "    \n",
    "    return gan, G, D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c1oyDNEOVHQZ"
   },
   "source": [
    "# <center> <font color=blue>    Utility functions for Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UyLI7SLZVHQZ"
   },
   "outputs": [],
   "source": [
    "def trainable(gan_model):\n",
    "    for layer in gan_model.layers:\n",
    "        layer.trainable = True\n",
    "\n",
    "def non_trainable(gan_model):\n",
    "    for layer in gan_model.layers:\n",
    "        layer.trainable = False       \n",
    "        \n",
    "def make_labels(batch_size, isReal):\n",
    "    if isReal==True:\n",
    "        return np.ones([batch_size, 1])\n",
    "    else:\n",
    "        return np.zeros([batch_size, 1])\n",
    "    \n",
    "''' Label Smoothing : From Soumith Chintalaâ€™s GAN Hacks \n",
    "    Ref : https://www.youtube.com/watch?v=X1mUN6dD8uE '''\n",
    "\n",
    "# Here, I use only one-sided smoothing\n",
    "# Smoothing Real images to [0.9, 1.0]\n",
    "def smooth_positive_labels(y):\n",
    "    return y - 0.1 + (random(y.shape)*0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ifeOlA4pVHQb"
   },
   "source": [
    "# Necessary text files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T9BScQ_bVHQc"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('iteration_count.txt'):\n",
    "    with open(\"iteration_count.txt\", \"a\") as f:\n",
    "        f.write(\"Iteration_count : \\n\")\n",
    "        \n",
    "if not os.path.exists('losses.txt'):\n",
    "    with open(\"losses.txt\", \"a\") as f:\n",
    "        f.write(\"D_Real_Loss D_Fake_Loss D_Total_loss G_Loss\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GtAYg9rsVHQd"
   },
   "source": [
    "# <center> <font color=red> TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EnQGiwVIVHQf"
   },
   "outputs": [],
   "source": [
    "def train_DCGAN(g_learning_rate,    # learning rate for the generator\n",
    "                g_beta_1,           # the exponential decay rate for the 1st moment estimates in Adam optimizer\n",
    "                d_learning_rate,    # learning rate for the discriminator\n",
    "                d_beta_1            # the exponential decay rate for the 1st moment estimates in Adam optimizer\n",
    "               ):\n",
    "\n",
    "    # Training specific Hyperparameter\n",
    "    max_iters = 160000\n",
    "    smooth= 0.05        # label smooting to avoid overfitting problem in discriminator model\n",
    "    batch_size = 64     # batch size\n",
    "    \n",
    "    # Phase iteration count \n",
    "    # Phase split ratio :-  T1:T2:T3 --> 18:2:80\n",
    "    phase_1_iters = 36000\n",
    "    phase_2_iters = 4000\n",
    "\n",
    "    # Get the previous iteration counts if the models was trained before\n",
    "    prev_iter = 0\n",
    "    \n",
    "    if os.path.exists('iteration_count.txt'):\n",
    "        prev_iter = sum(1 for line in open('iteration_count.txt'))-1\n",
    "    print('Previous iteration count :',prev_iter)\n",
    "    \n",
    "\n",
    "    # labels for real and fake images\n",
    "    y_train_real = make_labels(batch_size,True)\n",
    "    y_train_fake = make_labels(batch_size,False)\n",
    "    \n",
    "    \n",
    "    # create a GAN, a generator and a discriminator or load the previously trained models\n",
    "    if os.path.exists('best_generator.h5'):\n",
    "        generator     = load_model('best_generator.h5')\n",
    "        discriminator = load_model('best_discriminator.h5')\n",
    "        gan           = load_model('best_gan.h5')\n",
    "        print(\"Loaded old Gan, Generator and Discriminator\")\n",
    "      \n",
    "    else:\n",
    "        gan, generator, discriminator = create_DCGAN(g_learning_rate, g_beta_1, d_learning_rate, d_beta_1)\n",
    "        print(\"Created new Gan, Generator and Discriminator\")\n",
    "      \n",
    "    start_time = time.time()\n",
    "    \n",
    "    \n",
    "    for iters in range(prev_iter, max_iters):\n",
    "        print(\"Iteration :\",iters)\n",
    "        with open('iteration_count.txt', 'w') as f:\n",
    "            f.write(str(iters))\n",
    "        \n",
    "        # Training phase 01 - Train only the generator according to MSE loss\n",
    "        \n",
    "        if iters < phase_1_iters: \n",
    "            print(\"phase 01\")\n",
    "            discriminator.trainable = False \n",
    "            X_batch_masked = X_mask[np.random.choice(X_mask.shape[0], batch_size , replace = True), :] \n",
    "            gan.train_on_batch(X_batch_masked, y_train_real)\n",
    "    \n",
    "    \n",
    "        # Training phase 02 - Train only the discriminator \n",
    "        \n",
    "        elif iters < phase_1_iters + phase_2_iters:\n",
    "            print(\"phase 02\")\n",
    "            discriminator.trainable = True \n",
    "            X_batch_real   = X_train[np.random.choice(X_train.shape[0], batch_size , replace = True), :] \n",
    "            \n",
    "            X_batch_masked = X_mask[np.random.choice(X_mask.shape[0], batch_size , replace = True), :] \n",
    "            X_batch_fake   = generator.predict_on_batch(X_batch_masked)\n",
    "\n",
    "            discriminator.train_on_batch(X_batch_real, smooth_positive_labels(y_train_real))\n",
    "            discriminator.train_on_batch(X_batch_fake, y_train_fake)\n",
    "        \n",
    "        \n",
    "        # Training phase 03 - Train both D and G adversarially\n",
    "        else:\n",
    "            print(\"phase 03\")\n",
    "            if iters==phase_1_iters + phase_2_iters:\n",
    "                GAN,GENERATOR,_ = create_DCGAN(g_learning_rate, g_beta_1, d_learning_rate, d_beta_1, 2)\n",
    "                GAN.set_weights(gan.get_weights())\n",
    "                GENERATOR.set_weights(generator.get_weights())\n",
    "                \n",
    "            # TRAIN - DISCRIMINATOR\n",
    "            discriminator.trainable = True # trainable(discriminator)\n",
    "\n",
    "            # Real samples\n",
    "            X_batch_real   = X_train[np.random.choice(X_train.shape[0], batch_size , replace = True), :] \n",
    "\n",
    "            # Fake Samples\n",
    "            X_batch_masked = X_mask[np.random.choice(X_mask.shape[0], batch_size , replace = True), :] \n",
    "            X_batch_fake   = GENERATOR.predict_on_batch(X_batch_masked)\n",
    "\n",
    "            # Train the discriminator to detect real and fake images\n",
    "            discriminator.train_on_batch(X_batch_real, smooth_positive_labels(y_train_real))\n",
    "            discriminator.train_on_batch(X_batch_fake, y_train_fake)\n",
    "\n",
    "            # TRAIN - GENERATOR\n",
    "            discriminator.trainable = False # non_trainable(discriminator)\n",
    "            GAN.train_on_batch(X_batch_masked, y_train_real)\n",
    "        \n",
    "        \n",
    "        # Training Losses\n",
    "        \n",
    "        if (iters + 1) % 10000 == 0:\n",
    "            \n",
    "            X_batch_masked = X_mask[np.random.choice(X_mask.shape[0], batch_size , replace = True), :]\n",
    "            \n",
    "            if iters < phase_1_iters + phase_2_iters:\n",
    "                gan_images     = generator.predict_on_batch(X_batch_masked)\n",
    "                g_loss_batch   = gan.test_on_batch(X_batch_masked, y_train_real)\n",
    "            else:\n",
    "                gan_images     = GENERATOR.predict_on_batch(X_batch_masked)\n",
    "                g_loss_batch   = GAN.test_on_batch(X_batch_masked, y_train_real)\n",
    "\n",
    "            X_batch_real    = X_train[np.random.choice(X_train.shape[0], batch_size , replace = True), :] \n",
    "\n",
    "            d_loss_real  = discriminator.test_on_batch(X_batch_real, y_train_real)\n",
    "            d_loss_fake  = discriminator.test_on_batch(gan_images, y_train_fake)\n",
    "\n",
    "            with open(\"losses.txt\", \"a\") as f:\n",
    "                total = 0.5 * ( round(d_loss_real[0],12) + round(d_loss_fake[0],12) )\n",
    "                f.write( str(round(d_loss_real[0],12)) + ' ' + str(round(d_loss_fake[0],12)) +' ' + str(total) + ' ' + str( round(g_loss_batch[0],12) ) )\n",
    "                f.write('\\n')\n",
    "        \n",
    "\n",
    "        # CHECKPOINT FOR SAVING MODEL  \n",
    "        \n",
    "        if (iters + 1) % 10000 == 0 and iters > phase_1_iters + phase_2_iters:\n",
    "            if os.path.exists('best_generator.h5'):\n",
    "                os.remove('best_generator.h5')\n",
    "            GENERATOR.save('best_generator.h5')\n",
    "\n",
    "            if os.path.exists(\"best_discriminator.h5\"):\n",
    "                os.remove(\"best_discriminator.h5\")\n",
    "            discriminator.save('best_discriminator.h5')\n",
    "\n",
    "            if os.path.exists(\"best_gan.h5\"):\n",
    "                os.remove(\"best_gan.h5\")\n",
    "            GAN.save('best_gan.h5')\n",
    "            \n",
    "    print(\"Training Time:- %s seconds\" % (time.time() - start_time)) \n",
    "    \n",
    "    return GAN, GENERATOR, discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ttOiOW8YVHQi",
    "outputId": "59589725-454e-4df8-b990-872a52816b87"
   },
   "outputs": [],
   "source": [
    "GAN, GENERATOR, D = train_DCGAN(g_learning_rate=0.0001,g_beta_1=0.5,d_learning_rate=0.0001,d_beta_1=0.5)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Painting_Beyond_Boundaries.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
